{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvmsLcTEjs3JSTdrX719/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijeet06793/MNIST_DL/blob/master/MNIST_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD4BC6bWNXez"
      },
      "source": [
        "## **GAN on MNIST Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0rlrF1JM5aX"
      },
      "source": [
        "GAN is an unsupervised deep learning algorithm where we have a Generator network against an adversarial network called Discriminator network.\r\n",
        "Generator generates counterfeit currency. Discriminators are a team of cops trying to detect the counterfeit currency. Counterfeiters and cops both are trying to beat each other at their game.\r\n",
        "\r\n",
        "Both Generator and Discriminator will be multi-layer perceptrons(MLP)\r\n",
        "\r\n",
        "Generatorâ€™s objective will be to generate data that is very similar to the training data. Data generated from Generator should be indistinguishable from the real data.\r\n",
        "Discriminator takes two sets of input, one input comes from the training dataset(real data) and the other input is the dataset generated by Generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnD7AKZhNqeM"
      },
      "source": [
        "**Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWpi7pQxNpdj"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.layers import Dense, Dropout\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tqdm import tqdm\r\n",
        "from tensorflow.keras import Model\r\n",
        "from tensorflow.keras import Input"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lULqL1FQJDe"
      },
      "source": [
        "**Importing the MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM05K5GjNW5m",
        "outputId": "a183d531-9b9d-452d-b0e4-376af3e9946f"
      },
      "source": [
        "from keras.datasets import mnist\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "print(\"Initial shape\")\r\n",
        "print(x_train.shape)\r\n",
        "print(y_train.shape)\r\n",
        "\r\n",
        "x_train = x_train.reshape((60000, 784))/255\r\n",
        "x_test = x_test.reshape((10000, 784))/255\r\n",
        "print(\"Final shape\")\r\n",
        "print(x_train.shape)\r\n",
        "print(x_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Initial shape\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "Final shape\n",
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKc-uzELRnd8"
      },
      "source": [
        "### Generator\r\n",
        "\r\n",
        "We create Generator which uses MLP using simple dense layers activated by tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66u2yvU0NC1E"
      },
      "source": [
        "def create_generator():\r\n",
        "  generator = Sequential()\r\n",
        "  generator.add(Dense(units=256, input_dim = 100))\r\n",
        "  generator.add(LeakyReLU(alpha=0.2))\r\n",
        "  generator.add(Dense(units=512))\r\n",
        "  generator.add(LeakyReLU(alpha=0.2))\r\n",
        "  generator.add(Dense(units=1024))\r\n",
        "  generator.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  generator.add(Dense(units=784, activation='tanh'))\r\n",
        "\r\n",
        "  generator.compile(loss = 'binary_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "  return generator"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzg5EIWcREe6",
        "outputId": "294ac659-4829-45cc-dca0-a2d401206e05"
      },
      "source": [
        "g = create_generator()\r\n",
        "g.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 784)               803600    \n",
            "=================================================================\n",
            "Total params: 1,486,352\n",
            "Trainable params: 1,486,352\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiJ4rzlEUX9u"
      },
      "source": [
        "### **Descriminator**\r\n",
        "\r\n",
        "We now create the Discriminator which is also MLP. Discriminator will take the input from real data which is of the size 784 and also the images generated from Generator.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd_wWwS2REiJ",
        "outputId": "d5820704-4516-4eb7-ca78-18223cf5a516"
      },
      "source": [
        "def create_discriminator():\r\n",
        "  discriminator = Sequential()\r\n",
        "  discriminator.add(Dense(units=1024, input_dim=784))\r\n",
        "  discriminator.add(LeakyReLU(alpha=0.2))\r\n",
        "  discriminator.add(Dropout(rate = 0.3))\r\n",
        "\r\n",
        "  discriminator.add(Dense(units=512))\r\n",
        "  discriminator.add(LeakyReLU(alpha=0.2))\r\n",
        "  discriminator.add(Dropout(rate = 0.3))\r\n",
        "\r\n",
        "  discriminator.add(Dense(units=256))\r\n",
        "  discriminator.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  discriminator.add(Dense(units=1, activation='sigmoid'))\r\n",
        "  discriminator.compile(loss='binary_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "  return discriminator\r\n",
        "\r\n",
        "d = create_discriminator()\r\n",
        "d.summary()\r\n",
        "\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 1,460,225\n",
            "Trainable params: 1,460,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT4OjE80WFHV"
      },
      "source": [
        "We now create the GAN where we combine the Generator and Discriminator. When we train the generator we will freeze the Discriminator and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rVRL1LdREnY",
        "outputId": "954e49ac-1092-4aa6-9dba-d8bbe90fab99"
      },
      "source": [
        "#Note - We will input the noised image of shape 100 units to the Generator as an Input. \r\n",
        "#The output generated from the Generator(of dim - None,786) will be fed to the Discriminator.\r\n",
        "\r\n",
        "def create_gan(generator, discriminator):\r\n",
        "  discriminator.trainable = False #This will froze the discriminator layer\r\n",
        "  gan_input = Input(shape=(100,)) #Input is used to initialize a tensor. We have to give a tensor to a model. Also Model class takes a tensor as an input.\r\n",
        "  x = generator(gan_input)  #Here x is tensor that is returned of dim (None, 784) and this tensor will act an input to discriminator. \r\n",
        "  gan_output = discriminator(x) #Here gan_output is a tensor.\r\n",
        "  gan = Model(inputs=gan_input, outputs=gan_output)\r\n",
        "  gan.compile(loss='binary_crossentropy', optimizer = 'adam')\r\n",
        "  return gan\r\n",
        "\r\n",
        "gan = create_gan(g, d)\r\n",
        "gan.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 784)               1486352   \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 1)                 1460225   \n",
            "=================================================================\n",
            "Total params: 2,946,577\n",
            "Trainable params: 1,486,352\n",
            "Non-trainable params: 1,460,225\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmb4Eat4uju1"
      },
      "source": [
        "## **Plotting**\r\n",
        "\r\n",
        "Before we start training the model, we will write a function plot_generated_images to plot the generated images. This way we can see how the images are generated. We save the generated images to file that we can view later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPS6qy5KHgaR"
      },
      "source": [
        "def plot_generated_images(epoch, generator):\r\n",
        "  noise = np.random.normal(loc=0, scale=1, size=(100,100))\r\n",
        "  generated_image = generator.predict(noise)\r\n",
        "  generated_image= generated_image.reshape((100, 28, 28))\r\n",
        "  fig = plt.figure(figsize=(10,10)) \r\n",
        "  for i in range(0,100):\r\n",
        "    plt.subplot(10,10,i+1)\r\n",
        "    plt.imshow(generated_image[i], interpolation='nearest', cmap='gray')\r\n",
        "    plt.axis('off')\r\n",
        "\r\n",
        "  plt.tight_layout()\r\n",
        "  plt.savefig(fname= \"/content/Epochs_figure/epoch_{}\".format(epoch))\r\n",
        "  plt.close(fig)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmc3U4fNs8vw"
      },
      "source": [
        "### **Training**\r\n",
        "\r\n",
        "We finally start to train GAN. We will first have the full code for training GAN and then break it step by step for understanding how the training happens.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD-CVmmxsbkd"
      },
      "source": [
        "def training(epochs=1, batch_size=100):\r\n",
        "  \r\n",
        "  num_of_batches = int(x_train.shape[0]/batch_size)\r\n",
        "  generator = create_generator()\r\n",
        "  discriminator = create_discriminator()\r\n",
        "  gan_model = create_gan(generator, discriminator)\r\n",
        "\r\n",
        "  for e in range(1, epochs+1):\r\n",
        "    print(\"Epoch-{} :\".format(e))\r\n",
        "    for j in range(num_of_batches):\r\n",
        "      noise_batch = np.random.uniform(0,1,size=(100, 100))\r\n",
        "      generated_image = generator.predict(noice_batch)\r\n",
        "      image_batch = x_train[j*100:(j+1)*100]\r\n",
        "\r\n",
        "      x = np.concatenate((image_batch, generated_image))\r\n",
        "      y = np.zeros(2*batch_size)\r\n",
        "      y[0:batch_size] = 1\r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "     \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  return gan_model\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHaInP-sREp4",
        "outputId": "84089238-7aa0-439e-858a-d8bcd216a3d5"
      },
      "source": [
        "y =np.zeros(2*100)\r\n",
        "y[0:100] = 1\r\n",
        "y"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70UVSdHJREsB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmTUvSBaREuP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhEaK__lREwd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3WOnmLJREy5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6jXHXKJRE14"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}